{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf875b0",
   "metadata": {},
   "source": [
    "# üé® Chromasonic: Image-to-Music Conversion Demo üéµ\n",
    "\n",
    "**Welcome to Chromasonic!** This comprehensive interactive notebook demonstrates the complete multimodal ML pipeline that transforms images into beautiful melodies through color analysis, wavelength mapping, and advanced musical generation.\n",
    "\n",
    "## üß© What You'll Experience:\n",
    "1. **üé® Color Extraction** - Advanced algorithms (K-means, quantization) extract dominant colors  \n",
    "2. **üåà Wavelength Mapping** - Scientific conversion from RGB ‚Üí wavelengths ‚Üí musical frequencies\n",
    "3. **üéº Melody Generation** - ML models (Markov, LSTM, Transformer) create coherent musical sequences\n",
    "4. **üîä Audio Synthesis** - Multiple synthesis techniques render high-quality audio\n",
    "5. **üéØ Fusion Strategies** - Blend color-derived notes with AI-generated melodies\n",
    "6. **üìä Evaluation Metrics** - Comprehensive quality assessment of the conversion\n",
    "\n",
    "## üöÄ Complete Pipeline:\n",
    "```\n",
    "Image ‚Üí Color Analysis ‚Üí Wavelength Science ‚Üí AI Music Generation ‚Üí Audio Synthesis ‚Üí üéµ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b38d3",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our image-to-music pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Setup and Imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Chromasonic modules - Complete pipeline\n",
    "from chromasonic import ChromasonicPipeline\n",
    "from chromasonic.image_processing.loader import ImageLoader\n",
    "from chromasonic.color_analysis.extractor import ColorExtractor\n",
    "from chromasonic.wavelength_mapping.converter import WavelengthConverter\n",
    "from chromasonic.melody_generation.models import MelodyGenerator\n",
    "from chromasonic.audio_synthesis.synthesizer import AudioSynthesizer\n",
    "\n",
    "# Advanced modules\n",
    "from chromasonic.image_features import ImageFeatureExtractor, MusicalParameterPredictor\n",
    "from chromasonic.fusion import FusionLayer, AdaptiveFusion, FusionMode\n",
    "from chromasonic.chords_instruments import ChordProgressionGenerator, InstrumentSelector, ArrangementGenerator\n",
    "from chromasonic.render_midi import MidiRenderer, render_arrangement_to_midi\n",
    "from chromasonic.eval_metrics import ComprehensiveEvaluator, MusicalQualityMetrics, ColorMusicAlignmentMetrics\n",
    "\n",
    "# Audio and visualization\n",
    "try:\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "    HAS_AUDIO = True\n",
    "except ImportError:\n",
    "    HAS_AUDIO = False\n",
    "    print(\"‚ö†Ô∏è  Audio libraries not available - some features will be limited\")\n",
    "\n",
    "try:\n",
    "    import IPython.display as ipd\n",
    "    HAS_IPYTHON_AUDIO = True\n",
    "except ImportError:\n",
    "    HAS_IPYTHON_AUDIO = False\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéµ Chromasonic Demo Environment Ready!\")\n",
    "print(f\"üîä Audio support: {'‚úÖ' if HAS_AUDIO else '‚ùå'}\")  \n",
    "print(f\"üéß IPython audio: {'‚úÖ' if HAS_IPYTHON_AUDIO else '‚ùå'}\")\n",
    "\n",
    "# Create sample data directory\n",
    "sample_dir = Path('../data/images')\n",
    "sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize core pipeline\n",
    "print(\"\\nüöÄ Initializing Chromasonic Pipeline...\")\n",
    "pipeline = ChromasonicPipeline(\n",
    "    model_type=\"markov\",  # Start with fast Markov model\n",
    "    scale=\"major\",\n",
    "    tempo=120,\n",
    "    duration=20.0\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d401c4",
   "metadata": {},
   "source": [
    "## 2. Image Processing and Color Extraction\n",
    "\n",
    "Let's start by creating functions to load images and extract their dominant colors using machine learning clustering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e86faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Advanced Color Extraction and Analysis\n",
    "\n",
    "def demonstrate_color_extraction():\n",
    "    \"\"\"Comprehensive color extraction demonstration with multiple algorithms.\"\"\"\n",
    "    \n",
    "    # Create a sample gradient image for demonstration\n",
    "    print(\"üñºÔ∏è  Creating sample image for analysis...\")\n",
    "    width, height = 400, 300\n",
    "    sample_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Create a beautiful gradient with multiple colors\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            # Complex gradient with multiple color zones\n",
    "            r = int(255 * (j / width) * (1 - i / height))\n",
    "            g = int(255 * (i / height) * (j / width))  \n",
    "            b = int(255 * (1 - j / width) * (i / height))\n",
    "            sample_image[i, j] = [r, g, b]\n",
    "    \n",
    "    # Add some random colorful patches for complexity\n",
    "    np.random.seed(42)\n",
    "    for _ in range(50):\n",
    "        x, y = np.random.randint(0, width), np.random.randint(0, height)\n",
    "        size = np.random.randint(10, 30)\n",
    "        color = np.random.randint(0, 256, 3)\n",
    "        sample_image[max(0,y-size):min(height,y+size), \n",
    "                    max(0,x-size):min(width,x+size)] = color\n",
    "    \n",
    "    # Initialize color extractor\n",
    "    color_extractor = ColorExtractor()\n",
    "    \n",
    "    # Test different extraction methods\n",
    "    methods = ['kmeans', 'quantization', 'histogram']\n",
    "    num_colors = 8\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Display original image\n",
    "    axes[0, 0].imshow(sample_image)\n",
    "    axes[0, 0].set_title('üñºÔ∏è Original Image', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    colors_by_method = {}\n",
    "    \n",
    "    for idx, method in enumerate(methods):\n",
    "        print(f\"üîç Extracting colors using {method}...\")\n",
    "        \n",
    "        # Extract colors\n",
    "        colors = color_extractor.extract_colors(\n",
    "            sample_image, \n",
    "            num_colors=num_colors, \n",
    "            method=method\n",
    "        )\n",
    "        colors_by_method[method] = colors\n",
    "        \n",
    "        # Create color palette visualization\n",
    "        palette = np.array(colors).reshape(1, -1, 3)\n",
    "        axes[0, idx + 1].imshow(palette)\n",
    "        axes[0, idx + 1].set_title(f'üé® {method.capitalize()} Colors', fontweight='bold')\n",
    "        axes[0, idx + 1].axis('off')\n",
    "        \n",
    "        # Color harmony analysis\n",
    "        harmony_info = color_extractor.get_color_harmony(colors)\n",
    "        \n",
    "        # Display harmony metrics\n",
    "        axes[1, idx + 1].bar(\n",
    "            range(len(colors)), \n",
    "            [np.mean(color) for color in colors],\n",
    "            color=[f'#{r:02x}{g:02x}{b:02x}' for r, g, b in colors]\n",
    "        )\n",
    "        axes[1, idx + 1].set_title(f'Brightness ({method})')\n",
    "        axes[1, idx + 1].set_xlabel('Color Index')\n",
    "        axes[1, idx + 1].set_ylabel('Brightness')\n",
    "    \n",
    "    # Color harmony comparison\n",
    "    harmony_data = []\n",
    "    for method in methods:\n",
    "        colors = colors_by_method[method]\n",
    "        harmony = color_extractor.get_color_harmony(colors)\n",
    "        harmony_data.append({\n",
    "            'Method': method.capitalize(),\n",
    "            'Dominant Temp': harmony['dominant_temperature'],\n",
    "            'Avg Saturation': harmony['average_saturation'],\n",
    "            'Avg Brightness': harmony['average_brightness']\n",
    "        })\n",
    "    \n",
    "    harmony_df = pd.DataFrame(harmony_data)\n",
    "    \n",
    "    # Harmony comparison chart\n",
    "    axes[1, 0].axis('off')\n",
    "    table = axes[1, 0].table(\n",
    "        cellText=harmony_df.values,\n",
    "        colLabels=harmony_df.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 1.5)\n",
    "    axes[1, 0].set_title('üåà Color Harmony Analysis', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Color extraction analysis complete!\")\n",
    "    \n",
    "    # Calculate color weights for the best method (k-means)\n",
    "    best_colors = colors_by_method['kmeans']\n",
    "    weights = color_extractor.get_color_weights(sample_image, best_colors)\n",
    "    \n",
    "    print(f\"\\nüìä Color Weights (prominence in image):\")\n",
    "    for i, (color, weight) in enumerate(zip(best_colors, weights)):\n",
    "        print(f\"  Color {i+1}: RGB{color} - Weight: {weight:.2%}\")\n",
    "    \n",
    "    return sample_image, best_colors, weights\n",
    "\n",
    "# Run the demonstration\n",
    "sample_image, extracted_colors, color_weights = demonstrate_color_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac20576",
   "metadata": {},
   "source": [
    "## 3. Color to Wavelength Conversion\n",
    "\n",
    "Now we'll implement the scientific conversion from RGB colors to light wavelengths. This is where the magic begins - transforming visual perception into the electromagnetic spectrum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91261d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavelengthConverter:\n",
    "    \"\"\"Convert RGB colors to wavelengths using color science principles.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Visible light spectrum boundaries (nanometers)\n",
    "        self.min_wavelength = 380  # Violet\n",
    "        self.max_wavelength = 750  # Red\n",
    "        \n",
    "    def rgb_to_wavelength_hue_based(self, r, g, b):\n",
    "        \"\"\"Convert RGB to wavelength based on HSV hue.\"\"\"\n",
    "        # Normalize RGB values\n",
    "        r_norm = r / 255.0\n",
    "        g_norm = g / 255.0\n",
    "        b_norm = b / 255.0\n",
    "        \n",
    "        # Convert to HSV\n",
    "        max_val = max(r_norm, g_norm, b_norm)\n",
    "        min_val = min(r_norm, g_norm, b_norm)\n",
    "        diff = max_val - min_val\n",
    "        \n",
    "        if diff == 0:\n",
    "            hue = 0\n",
    "        elif max_val == r_norm:\n",
    "            hue = (60 * ((g_norm - b_norm) / diff) + 360) % 360\n",
    "        elif max_val == g_norm:\n",
    "            hue = (60 * ((b_norm - r_norm) / diff) + 120) % 360\n",
    "        else:\n",
    "            hue = (60 * ((r_norm - g_norm) / diff) + 240) % 360\n",
    "        \n",
    "        # Map hue to wavelength\n",
    "        return self._hue_to_wavelength(hue)\n",
    "    \n",
    "    def _hue_to_wavelength(self, hue):\n",
    "        \"\"\"Map HSV hue (0-360¬∞) to wavelength (380-750 nm).\"\"\"\n",
    "        # Color hue to wavelength mapping based on visible spectrum\n",
    "        if 0 <= hue <= 60:  # Red to Yellow\n",
    "            return 700 - (hue / 60) * (700 - 580)\n",
    "        elif 60 < hue <= 120:  # Yellow to Green  \n",
    "            return 580 - ((hue - 60) / 60) * (580 - 520)\n",
    "        elif 120 < hue <= 180:  # Green to Cyan\n",
    "            return 520 - ((hue - 120) / 60) * (520 - 490)\n",
    "        elif 180 < hue <= 240:  # Cyan to Blue\n",
    "            return 490 - ((hue - 180) / 60) * (490 - 450)\n",
    "        elif 240 < hue <= 300:  # Blue to Magenta\n",
    "            return 450 - ((hue - 240) / 60) * (450 - 400)\n",
    "        else:  # Magenta to Red\n",
    "            return 400 + ((hue - 300) / 60) * (700 - 400)\n",
    "    \n",
    "    def wavelength_to_rgb_approximation(self, wavelength):\n",
    "        \"\"\"Convert wavelength back to approximate RGB (for validation).\"\"\"\n",
    "        if wavelength < 380 or wavelength > 750:\n",
    "            return (0, 0, 0)\n",
    "        \n",
    "        # Dan Bruton's wavelength to RGB approximation\n",
    "        if 380 <= wavelength <= 440:\n",
    "            red = -(wavelength - 440) / (440 - 380)\n",
    "            green = 0.0\n",
    "            blue = 1.0\n",
    "        elif 440 <= wavelength <= 490:\n",
    "            red = 0.0\n",
    "            green = (wavelength - 440) / (490 - 440)\n",
    "            blue = 1.0\n",
    "        elif 490 <= wavelength <= 510:\n",
    "            red = 0.0\n",
    "            green = 1.0\n",
    "            blue = -(wavelength - 510) / (510 - 490)\n",
    "        elif 510 <= wavelength <= 580:\n",
    "            red = (wavelength - 510) / (580 - 510)\n",
    "            green = 1.0\n",
    "            blue = 0.0\n",
    "        elif 580 <= wavelength <= 645:\n",
    "            red = 1.0\n",
    "            green = -(wavelength - 645) / (645 - 580)\n",
    "            blue = 0.0\n",
    "        elif 645 <= wavelength <= 750:\n",
    "            red = 1.0\n",
    "            green = 0.0\n",
    "            blue = 0.0\n",
    "        \n",
    "        # Intensity correction near vision limits\n",
    "        if 380 <= wavelength <= 420:\n",
    "            factor = 0.3 + 0.7 * (wavelength - 380) / (420 - 380)\n",
    "        elif 420 <= wavelength <= 700:\n",
    "            factor = 1.0\n",
    "        elif 700 <= wavelength <= 750:\n",
    "            factor = 0.3 + 0.7 * (750 - wavelength) / (750 - 700)\n",
    "        else:\n",
    "            factor = 0.0\n",
    "        \n",
    "        # Convert to 8-bit RGB\n",
    "        r = int(255 * red * factor) if red > 0 else 0\n",
    "        g = int(255 * green * factor) if green > 0 else 0\n",
    "        b = int(255 * blue * factor) if blue > 0 else 0\n",
    "        \n",
    "        return (r, g, b)\n",
    "    \n",
    "    def visualize_spectrum_mapping(self, colors, figsize=(15, 8)):\n",
    "        \"\"\"Visualize the color-to-wavelength mapping.\"\"\"\n",
    "        wavelengths = []\n",
    "        spectrum_colors = []\n",
    "        \n",
    "        for color in colors:\n",
    "            wl = self.rgb_to_wavelength_hue_based(color[0], color[1], color[2])\n",
    "            wavelengths.append(wl)\n",
    "            spectrum_colors.append(self.wavelength_to_rgb_approximation(wl))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # Original colors\n",
    "        color_bar = np.array(colors)[np.newaxis, :, :] / 255.0\n",
    "        axes[0, 0].imshow(color_bar)\n",
    "        axes[0, 0].set_title('Original Colors', fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Corresponding spectrum colors\n",
    "        spectrum_bar = np.array(spectrum_colors)[np.newaxis, :, :] / 255.0\n",
    "        axes[0, 1].imshow(spectrum_bar)\n",
    "        axes[0, 1].set_title('Spectrum Approximation', fontweight='bold')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Wavelength values\n",
    "        x_pos = np.arange(len(colors))\n",
    "        bars = axes[1, 0].bar(x_pos, wavelengths, color=[c/255.0 for c in colors])\n",
    "        axes[1, 0].set_title('Wavelengths (nm)', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Color Index')\n",
    "        axes[1, 0].set_ylabel('Wavelength (nm)')\n",
    "        axes[1, 0].set_ylim(350, 800)\n",
    "        \n",
    "        # Add wavelength labels on bars\n",
    "        for i, (bar, wl) in enumerate(zip(bars, wavelengths)):\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                           f'{wl:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Visible spectrum reference\n",
    "        spectrum_range = np.linspace(380, 750, 371)\n",
    "        spectrum_rgb = np.array([self.wavelength_to_rgb_approximation(wl) for wl in spectrum_range])\n",
    "        spectrum_img = spectrum_rgb.reshape(1, -1, 3) / 255.0\n",
    "        \n",
    "        axes[1, 1].imshow(spectrum_img, extent=[380, 750, -0.5, 0.5])\n",
    "        axes[1, 1].set_title('Visible Spectrum Reference', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Wavelength (nm)')\n",
    "        axes[1, 1].set_yticks([])\n",
    "        \n",
    "        # Mark our extracted wavelengths\n",
    "        for wl in wavelengths:\n",
    "            axes[1, 1].axvline(x=wl, color='white', linestyle='--', alpha=0.8, linewidth=2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return wavelengths, fig\n",
    "\n",
    "# Test the wavelength conversion\n",
    "converter = WavelengthConverter()\n",
    "\n",
    "print(\"üåà Converting colors to wavelengths...\")\n",
    "wavelengths, fig = converter.visualize_spectrum_mapping(colors)\n",
    "\n",
    "print(\"Wavelength mapping results:\")\n",
    "for i, (color, wl) in enumerate(zip(colors, wavelengths)):\n",
    "    print(f\"  Color {i+1}: RGB{tuple(color)} ‚Üí {wl:.1f} nm\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601eb8c",
   "metadata": {},
   "source": [
    "## 4. Wavelength to Musical Frequency Mapping\n",
    "\n",
    "This is where science meets art! We'll convert light wavelengths to sound frequencies using creative mathematical mappings that preserve the harmonic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyMapper:\n",
    "    \"\"\"Map wavelengths to musical frequencies.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.speed_of_light = 299792458  # m/s\n",
    "        self.a4_frequency = 440.0  # Hz (A4 reference)\n",
    "        self.a4_midi_note = 69    # MIDI note for A4\n",
    "    \n",
    "    def wavelengths_to_frequencies(self, wavelengths, octave_range=(3, 6), method='linear'):\n",
    "        \"\"\"Convert wavelengths to musical frequencies.\"\"\"\n",
    "        musical_frequencies = []\n",
    "        \n",
    "        # Define frequency range for the octaves\n",
    "        min_freq = self._midi_to_frequency(octave_range[0] * 12)  # C of min octave\n",
    "        max_freq = self._midi_to_frequency(octave_range[1] * 12)  # C of max octave\n",
    "        \n",
    "        # Normalize wavelengths to frequency range\n",
    "        min_wl = min(wavelengths)\n",
    "        max_wl = max(wavelengths)\n",
    "        \n",
    "        for wl in wavelengths:\n",
    "            if method == 'linear':\n",
    "                # Linear mapping from wavelength range to frequency range\n",
    "                normalized = (wl - min_wl) / (max_wl - min_wl) if max_wl != min_wl else 0.5\n",
    "                # Invert so shorter wavelengths (blue) = higher frequencies\n",
    "                freq = min_freq + (1 - normalized) * (max_freq - min_freq)\n",
    "            \n",
    "            elif method == 'logarithmic':\n",
    "                # Logarithmic mapping (more musical)\n",
    "                normalized = (wl - min_wl) / (max_wl - min_wl) if max_wl != min_wl else 0.5\n",
    "                freq = min_freq * ((max_freq / min_freq) ** (1 - normalized))\n",
    "            \n",
    "            elif method == 'harmonic':\n",
    "                # Based on harmonic series\n",
    "                base_freq = 220.0  # A3\n",
    "                harmonic = int(1 + (750 - wl) / (750 - 380) * 15)  # 1-16 harmonics\n",
    "                freq = base_freq * harmonic\n",
    "                \n",
    "            musical_frequencies.append(freq)\n",
    "        \n",
    "        return musical_frequencies\n",
    "    \n",
    "    def _midi_to_frequency(self, midi_note):\n",
    "        \"\"\"Convert MIDI note number to frequency.\"\"\"\n",
    "        return self.a4_frequency * (2 ** ((midi_note - self.a4_midi_note) / 12))\n",
    "    \n",
    "    def frequency_to_midi_note(self, frequency):\n",
    "        \"\"\"Convert frequency to MIDI note number.\"\"\"\n",
    "        return int(69 + 12 * np.log2(frequency / self.a4_frequency))\n",
    "    \n",
    "    def frequency_to_note_name(self, frequency):\n",
    "        \"\"\"Convert frequency to note name.\"\"\"\n",
    "        midi_note = self.frequency_to_midi_note(frequency)\n",
    "        note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "        octave = (midi_note // 12) - 1\n",
    "        note = note_names[midi_note % 12]\n",
    "        return f\"{note}{octave}\"\n",
    "    \n",
    "    def visualize_frequency_mapping(self, wavelengths, frequencies, colors, figsize=(15, 10)):\n",
    "        \"\"\"Visualize the wavelength to frequency conversion.\"\"\"\n",
    "        fig, axes = plt.subplots(3, 2, figsize=figsize)\n",
    "        \n",
    "        # Wavelength vs Frequency scatter plot\n",
    "        scatter = axes[0, 0].scatter(wavelengths, frequencies, c=[c/255.0 for c in colors], s=100)\n",
    "        axes[0, 0].set_xlabel('Wavelength (nm)')\n",
    "        axes[0, 0].set_ylabel('Frequency (Hz)')\n",
    "        axes[0, 0].set_title('Wavelength ‚Üí Frequency Mapping', fontweight='bold')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Frequency bars with note names\n",
    "        note_names = [self.frequency_to_note_name(f) for f in frequencies]\n",
    "        x_pos = np.arange(len(frequencies))\n",
    "        bars = axes[0, 1].bar(x_pos, frequencies, color=[c/255.0 for c in colors])\n",
    "        axes[0, 1].set_title('Musical Frequencies', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Color Index')\n",
    "        axes[0, 1].set_ylabel('Frequency (Hz)')\n",
    "        axes[0, 1].set_xticks(x_pos)\n",
    "        axes[0, 1].set_xticklabels([f'C{i+1}' for i in range(len(frequencies))])\n",
    "        \n",
    "        # Add frequency and note labels\n",
    "        for i, (bar, freq, note) in enumerate(zip(bars, frequencies, note_names)):\n",
    "            axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "                           f'{freq:.0f}Hz\\\\n{note}', ha='center', va='bottom', \n",
    "                           fontsize=8, fontweight='bold')\n",
    "        \n",
    "        # Piano keyboard visualization\n",
    "        self._draw_piano_keyboard(axes[1, :], frequencies, note_names, colors)\n",
    "        \n",
    "        # Frequency comparison methods\n",
    "        methods = ['linear', 'logarithmic', 'harmonic']\n",
    "        freq_comparisons = []\n",
    "        \n",
    "        for method in methods:\n",
    "            method_freqs = self.wavelengths_to_frequencies(wavelengths, method=method)\n",
    "            freq_comparisons.append(method_freqs)\n",
    "        \n",
    "        for i, (method, freqs) in enumerate(zip(methods, freq_comparisons)):\n",
    "            axes[2, 0].plot(wavelengths, freqs, 'o-', label=method.title(), markersize=8)\n",
    "        \n",
    "        axes[2, 0].set_xlabel('Wavelength (nm)')\n",
    "        axes[2, 0].set_ylabel('Frequency (Hz)')\n",
    "        axes[2, 0].set_title('Mapping Method Comparison', fontweight='bold')\n",
    "        axes[2, 0].legend()\n",
    "        axes[2, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Musical interval analysis\n",
    "        intervals = []\n",
    "        for i in range(1, len(frequencies)):\n",
    "            ratio = frequencies[i] / frequencies[i-1]\n",
    "            semitones = 12 * np.log2(ratio)\n",
    "            intervals.append(semitones)\n",
    "        \n",
    "        if intervals:\n",
    "            axes[2, 1].bar(range(len(intervals)), intervals, \n",
    "                          color=[c/255.0 for c in colors[1:]])\n",
    "            axes[2, 1].set_title('Musical Intervals (Semitones)', fontweight='bold')\n",
    "            axes[2, 1].set_xlabel('Color Transition')\n",
    "            axes[2, 1].set_ylabel('Interval (Semitones)')\n",
    "            axes[2, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return frequencies, note_names, fig\n",
    "    \n",
    "    def _draw_piano_keyboard(self, axes, frequencies, note_names, colors):\n",
    "        \"\"\"Draw a piano keyboard showing the mapped notes.\"\"\"\n",
    "        # Combine both subplot areas for the keyboard\n",
    "        keyboard_ax = plt.subplot2grid((3, 2), (1, 0), colspan=2)\n",
    "        \n",
    "        # Define keyboard layout\n",
    "        white_keys = ['C', 'D', 'E', 'F', 'G', 'A', 'B']\n",
    "        black_keys = ['C#', 'D#', '', 'F#', 'G#', 'A#', '']\n",
    "        \n",
    "        # Draw white keys\n",
    "        white_width = 1.0\n",
    "        white_height = 5.0\n",
    "        \n",
    "        for i in range(21):  # 3 octaves\n",
    "            x = i * white_width\n",
    "            key_rect = plt.Rectangle((x, 0), white_width, white_height, \n",
    "                                   facecolor='white', edgecolor='black', linewidth=1)\n",
    "            keyboard_ax.add_patch(key_rect)\n",
    "        \n",
    "        # Draw black keys\n",
    "        black_width = 0.6\n",
    "        black_height = 3.0\n",
    "        black_positions = [0.7, 1.7, 3.7, 4.7, 5.7]  # Positions within each octave\n",
    "        \n",
    "        for octave in range(3):\n",
    "            for pos in black_positions:\n",
    "                x = octave * 7 + pos\n",
    "                key_rect = plt.Rectangle((x, black_height), black_width, \n",
    "                                       white_height - black_height,\n",
    "                                       facecolor='black', edgecolor='black')\n",
    "                keyboard_ax.add_patch(key_rect)\n",
    "        \n",
    "        # Highlight our mapped notes\n",
    "        for freq, note, color in zip(frequencies, note_names, colors):\n",
    "            # Find the position on keyboard (simplified)\n",
    "            note_base = note[:-1]  # Remove octave number\n",
    "            octave = int(note[-1])\n",
    "            \n",
    "            if note_base in white_keys:\n",
    "                key_pos = (octave - 3) * 7 + white_keys.index(note_base)\n",
    "                highlight_rect = plt.Rectangle((key_pos * white_width, 0), \n",
    "                                             white_width, white_height,\n",
    "                                             facecolor=tuple(color/255.0), alpha=0.7)\n",
    "                keyboard_ax.add_patch(highlight_rect)\n",
    "                # Add frequency label\n",
    "                keyboard_ax.text(key_pos * white_width + white_width/2, white_height/2,\n",
    "                               f'{freq:.0f}', ha='center', va='center', \n",
    "                               fontweight='bold', fontsize=8)\n",
    "        \n",
    "        keyboard_ax.set_xlim(0, 21)\n",
    "        keyboard_ax.set_ylim(0, white_height)\n",
    "        keyboard_ax.set_aspect('equal')\n",
    "        keyboard_ax.set_title('Piano Keyboard Mapping', fontweight='bold', pad=20)\n",
    "        keyboard_ax.axis('off')\n",
    "\n",
    "# Test frequency mapping\n",
    "mapper = FrequencyMapper()\n",
    "\n",
    "print(\"üéµ Converting wavelengths to musical frequencies...\")\n",
    "frequencies, note_names, fig = mapper.visualize_frequency_mapping(wavelengths, \n",
    "                                                                 mapper.wavelengths_to_frequencies(wavelengths),\n",
    "                                                                 colors)\n",
    "\n",
    "print(\"\\\\nFrequency mapping results:\")\n",
    "for i, (wl, freq, note, color) in enumerate(zip(wavelengths, frequencies, note_names, colors)):\n",
    "    print(f\"  Color {i+1}: {wl:.1f}nm ‚Üí {freq:.1f}Hz ({note}) | RGB{tuple(color)}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87144f4e",
   "metadata": {},
   "source": [
    "## 5. Musical Scale Implementation\n",
    "\n",
    "Now let's implement various musical scales and create functions to quantize our frequencies to create harmonious melodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68679606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicalScales:\n",
    "    \"\"\"Handle various musical scales and note quantization.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define scales as semitone intervals from root note\n",
    "        self.scales = {\n",
    "            'major': [0, 2, 4, 5, 7, 9, 11],\n",
    "            'minor': [0, 2, 3, 5, 7, 8, 10],\n",
    "            'pentatonic': [0, 2, 4, 7, 9],\n",
    "            'blues': [0, 3, 5, 6, 7, 10],\n",
    "            'chromatic': list(range(12)),\n",
    "            'dorian': [0, 2, 3, 5, 7, 9, 10],\n",
    "            'mixolydian': [0, 2, 4, 5, 7, 9, 10],\n",
    "            'harmonic_minor': [0, 2, 3, 5, 7, 8, 11],\n",
    "            'whole_tone': [0, 2, 4, 6, 8, 10]\n",
    "        }\n",
    "        \n",
    "        # Scale descriptions\n",
    "        self.descriptions = {\n",
    "            'major': 'Happy, bright, uplifting',\n",
    "            'minor': 'Sad, melancholic, introspective',\n",
    "            'pentatonic': 'Universal, pleasing, ancient',\n",
    "            'blues': 'Expressive, soulful, emotional',\n",
    "            'chromatic': 'All 12 semitones, modern',\n",
    "            'dorian': 'Modal, mysterious, Celtic',\n",
    "            'mixolydian': 'Folk, rustic, medieval',\n",
    "            'harmonic_minor': 'Exotic, Middle Eastern',\n",
    "            'whole_tone': 'Dreamy, impressionistic'\n",
    "        }\n",
    "    \n",
    "    def quantize_to_scale(self, frequencies, scale_name='major', root_freq=261.63):\n",
    "        \"\"\"Quantize frequencies to the nearest notes in a musical scale.\"\"\"\n",
    "        if scale_name not in self.scales:\n",
    "            scale_name = 'major'\n",
    "        \n",
    "        scale_intervals = self.scales[scale_name]\n",
    "        quantized_freqs = []\n",
    "        scale_notes = []\n",
    "        \n",
    "        for freq in frequencies:\n",
    "            # Convert frequency to MIDI note number\n",
    "            midi_note = 69 + 12 * np.log2(freq / 440.0)\n",
    "            \n",
    "            # Find the closest scale note\n",
    "            note_in_octave = midi_note % 12\n",
    "            \n",
    "            closest_interval = min(scale_intervals, \n",
    "                                 key=lambda x: min(abs(note_in_octave - x), \n",
    "                                                  abs(note_in_octave - x - 12),\n",
    "                                                  abs(note_in_octave - x + 12)))\n",
    "            \n",
    "            # Calculate the quantized MIDI note\n",
    "            octave = int(midi_note // 12)\n",
    "            quantized_midi = octave * 12 + closest_interval\n",
    "            \n",
    "            # Convert back to frequency\n",
    "            quantized_freq = 440.0 * (2 ** ((quantized_midi - 69) / 12))\n",
    "            quantized_freqs.append(quantized_freq)\n",
    "            \n",
    "            # Store scale note index\n",
    "            scale_notes.append(scale_intervals.index(closest_interval))\n",
    "        \n",
    "        return quantized_freqs, scale_notes\n",
    "    \n",
    "    def generate_chord_progression(self, scale_notes, scale_name='major'):\n",
    "        \"\"\"Generate a simple chord progression from scale notes.\"\"\"\n",
    "        scale_intervals = self.scales[scale_name]\n",
    "        \n",
    "        # Common chord progressions for major scales\n",
    "        progressions = {\n",
    "            'major': [[0, 2, 4], [5, 0, 2], [3, 5, 0], [0]],  # I-vi-IV-I\n",
    "            'minor': [[0, 2, 4], [5, 0, 2], [1, 3, 5], [0]],  # i-VI-III-i\n",
    "            'pentatonic': [[0, 2, 4], [2, 4, 0], [4, 0, 2], [0]],\n",
    "            'blues': [[0, 3, 5], [3, 5, 0], [5, 0, 3], [0]]\n",
    "        }\n",
    "        \n",
    "        progression = progressions.get(scale_name, progressions['major'])\n",
    "        chord_sequence = []\n",
    "        \n",
    "        for chord_indices in progression:\n",
    "            chord = []\n",
    "            for idx in chord_indices:\n",
    "                if idx < len(scale_notes):\n",
    "                    chord.append(scale_notes[idx])\n",
    "            chord_sequence.append(chord)\n",
    "        \n",
    "        return chord_sequence\n",
    "    \n",
    "    def visualize_scales(self, frequencies, figsize=(16, 12)):\n",
    "        \"\"\"Compare how frequencies map to different scales.\"\"\"\n",
    "        fig, axes = plt.subplots(3, 3, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        scale_names = list(self.scales.keys())\n",
    "        \n",
    "        for i, scale_name in enumerate(scale_names):\n",
    "            if i >= 9:  # Limit to 9 scales for visualization\n",
    "                break\n",
    "                \n",
    "            quantized_freqs, scale_notes = self.quantize_to_scale(frequencies, scale_name)\n",
    "            \n",
    "            # Plot original vs quantized frequencies\n",
    "            x_pos = np.arange(len(frequencies))\n",
    "            \n",
    "            axes[i].bar(x_pos - 0.2, frequencies, width=0.4, \n",
    "                       label='Original', alpha=0.7, color='lightblue')\n",
    "            axes[i].bar(x_pos + 0.2, quantized_freqs, width=0.4, \n",
    "                       label='Quantized', alpha=0.7, color='orange')\n",
    "            \n",
    "            axes[i].set_title(f'{scale_name.title()}\\\\n{self.descriptions[scale_name]}',\n",
    "                             fontweight='bold', fontsize=10)\n",
    "            axes[i].set_ylabel('Frequency (Hz)')\n",
    "            axes[i].legend(fontsize=8)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add note names\n",
    "            for j, (orig_freq, quant_freq) in enumerate(zip(frequencies, quantized_freqs)):\n",
    "                note_name = mapper.frequency_to_note_name(quant_freq)\n",
    "                axes[i].text(j, quant_freq + 50, note_name, \n",
    "                           ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Test scale quantization\n",
    "scales = MusicalScales()\n",
    "\n",
    "print(\"üéº Quantizing frequencies to different musical scales...\")\n",
    "\n",
    "# Test with major scale\n",
    "quantized_freqs, scale_notes = scales.quantize_to_scale(frequencies, 'major')\n",
    "print(f\"\\\\nMajor scale quantization:\")\n",
    "for i, (orig, quant) in enumerate(zip(frequencies, quantized_freqs)):\n",
    "    orig_note = mapper.frequency_to_note_name(orig)\n",
    "    quant_note = mapper.frequency_to_note_name(quant)\n",
    "    print(f\"  {orig:.1f}Hz ({orig_note}) ‚Üí {quant:.1f}Hz ({quant_note})\")\n",
    "\n",
    "# Visualize all scales\n",
    "fig = scales.visualize_scales(frequencies)\n",
    "plt.show()\n",
    "\n",
    "# Generate a chord progression\n",
    "chord_progression = scales.generate_chord_progression(scale_notes, 'major')\n",
    "print(f\"\\\\nüéπ Generated chord progression: {chord_progression}\")\n",
    "\n",
    "# Compare different scales\n",
    "print(\"\\\\nüéµ Scale comparison:\")\n",
    "for scale_name in ['major', 'minor', 'pentatonic', 'blues']:\n",
    "    quant_freqs, _ = scales.quantize_to_scale(frequencies, scale_name)\n",
    "    notes = [mapper.frequency_to_note_name(f) for f in quant_freqs]\n",
    "    print(f\"  {scale_name.title()}: {' - '.join(notes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23090738",
   "metadata": {},
   "source": [
    "## 6. Audio Synthesis and Playback\n",
    "\n",
    "Let's create beautiful audio from our musical data using various synthesis techniques!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSynthesizer:\n",
    "    \"\"\"Synthesize audio from musical frequencies.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=44100):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def generate_tone(self, frequency, duration, method='sine', envelope=True):\n",
    "        \"\"\"Generate a single tone using different synthesis methods.\"\"\"\n",
    "        num_samples = int(duration * self.sample_rate)\n",
    "        t = np.linspace(0, duration, num_samples, False)\n",
    "        \n",
    "        if method == 'sine':\n",
    "            # Pure sine wave\n",
    "            audio = np.sin(2 * np.pi * frequency * t)\n",
    "        \n",
    "        elif method == 'additive':\n",
    "            # Additive synthesis with harmonics\n",
    "            audio = np.sin(2 * np.pi * frequency * t)\n",
    "            # Add harmonics with decreasing amplitude\n",
    "            harmonics = [2, 3, 4, 5]\n",
    "            amplitudes = [0.5, 0.25, 0.125, 0.0625]\n",
    "            \n",
    "            for harmonic, amplitude in zip(harmonics, amplitudes):\n",
    "                audio += amplitude * np.sin(2 * np.pi * frequency * harmonic * t)\n",
    "        \n",
    "        elif method == 'fm':\n",
    "            # FM synthesis\n",
    "            modulator_freq = frequency * 2\n",
    "            modulation_index = 5\n",
    "            modulator = modulation_index * np.sin(2 * np.pi * modulator_freq * t)\n",
    "            audio = np.sin(2 * np.pi * frequency * t + modulator)\n",
    "        \n",
    "        elif method == 'sawtooth':\n",
    "            # Sawtooth wave\n",
    "            audio = 2 * (t * frequency - np.floor(t * frequency + 0.5))\n",
    "        \n",
    "        # Apply ADSR envelope\n",
    "        if envelope:\n",
    "            audio = self._apply_envelope(audio, duration)\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    def _apply_envelope(self, audio, duration, attack=0.1, decay=0.1, sustain=0.7, release=0.2):\n",
    "        \"\"\"Apply ADSR envelope to audio.\"\"\"\n",
    "        num_samples = len(audio)\n",
    "        envelope = np.ones(num_samples)\n",
    "        \n",
    "        # Calculate sample boundaries\n",
    "        attack_samples = int(attack * self.sample_rate)\n",
    "        decay_samples = int(decay * self.sample_rate)\n",
    "        release_samples = int(release * self.sample_rate)\n",
    "        \n",
    "        # Ensure envelope fits within audio length\n",
    "        total_envelope_samples = attack_samples + decay_samples + release_samples\n",
    "        if total_envelope_samples > num_samples:\n",
    "            # Scale down envelope times\n",
    "            scale_factor = num_samples / total_envelope_samples\n",
    "            attack_samples = int(attack_samples * scale_factor)\n",
    "            decay_samples = int(decay_samples * scale_factor)\n",
    "            release_samples = int(release_samples * scale_factor)\n",
    "        \n",
    "        sustain_samples = num_samples - attack_samples - decay_samples - release_samples\n",
    "        \n",
    "        idx = 0\n",
    "        \n",
    "        # Attack\n",
    "        if attack_samples > 0:\n",
    "            envelope[idx:idx+attack_samples] = np.linspace(0, 1, attack_samples)\n",
    "            idx += attack_samples\n",
    "        \n",
    "        # Decay\n",
    "        if decay_samples > 0:\n",
    "            envelope[idx:idx+decay_samples] = np.linspace(1, sustain, decay_samples)\n",
    "            idx += decay_samples\n",
    "        \n",
    "        # Sustain\n",
    "        if sustain_samples > 0:\n",
    "            envelope[idx:idx+sustain_samples] = sustain\n",
    "            idx += sustain_samples\n",
    "        \n",
    "        # Release\n",
    "        if release_samples > 0:\n",
    "            envelope[idx:idx+release_samples] = np.linspace(sustain, 0, release_samples)\n",
    "        \n",
    "        return audio * envelope\n",
    "    \n",
    "    def create_melody(self, frequencies, note_duration=0.5, method='additive'):\n",
    "        \"\"\"Create a melody from a sequence of frequencies.\"\"\"\n",
    "        melody_audio = []\n",
    "        \n",
    "        for freq in frequencies:\n",
    "            note = self.generate_tone(freq, note_duration, method)\n",
    "            melody_audio.append(note)\n",
    "        \n",
    "        # Concatenate all notes\n",
    "        return np.concatenate(melody_audio)\n",
    "    \n",
    "    def create_chord(self, frequencies, duration=2.0, method='additive'):\n",
    "        \"\"\"Create a chord from multiple frequencies played simultaneously.\"\"\"\n",
    "        chord_audio = np.zeros(int(duration * self.sample_rate))\n",
    "        \n",
    "        for freq in frequencies:\n",
    "            note = self.generate_tone(freq, duration, method)\n",
    "            chord_audio += note * (1.0 / len(frequencies))  # Normalize amplitude\n",
    "        \n",
    "        return chord_audio\n",
    "    \n",
    "    def add_reverb(self, audio, delay_time=0.1, decay=0.5):\n",
    "        \"\"\"Add simple reverb effect.\"\"\"\n",
    "        delay_samples = int(delay_time * self.sample_rate)\n",
    "        reverb_audio = audio.copy()\n",
    "        \n",
    "        if delay_samples < len(audio):\n",
    "            delayed = np.zeros_like(audio)\n",
    "            delayed[delay_samples:] = audio[:-delay_samples] * decay\n",
    "            reverb_audio += delayed\n",
    "        \n",
    "        return reverb_audio\n",
    "    \n",
    "    def visualize_waveform(self, audio, title=\"Audio Waveform\", figsize=(12, 4)):\n",
    "        \"\"\"Visualize audio waveform.\"\"\"\n",
    "        time = np.linspace(0, len(audio) / self.sample_rate, len(audio))\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(time, audio, linewidth=0.5)\n",
    "        plt.title(title, fontweight='bold')\n",
    "        plt.xlabel('Time (seconds)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "# Create synthesizer and generate audio\n",
    "synthesizer = AudioSynthesizer()\n",
    "\n",
    "print(\"üéµ Generating audio from our color-derived frequencies...\")\n",
    "\n",
    "# Create melodies with different synthesis methods\n",
    "methods = ['sine', 'additive', 'fm', 'sawtooth']\n",
    "melodies = {}\n",
    "\n",
    "for method in methods:\n",
    "    melody = synthesizer.create_melody(quantized_freqs, note_duration=0.8, method=method)\n",
    "    melodies[method] = melody\n",
    "    \n",
    "    print(f\"‚úÖ Generated {method} melody: {len(melody)/synthesizer.sample_rate:.1f} seconds\")\n",
    "\n",
    "# Visualize waveforms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method, audio) in enumerate(melodies.items()):\n",
    "    time = np.linspace(0, len(audio) / synthesizer.sample_rate, len(audio))\n",
    "    axes[i].plot(time, audio, linewidth=0.5, color=plt.cm.viridis(i/4))\n",
    "    axes[i].set_title(f'{method.title()} Synthesis', fontweight='bold')\n",
    "    axes[i].set_xlabel('Time (s)')\n",
    "    axes[i].set_ylabel('Amplitude')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create and play individual notes\n",
    "print(\"\\\\nüéº Individual note synthesis:\")\n",
    "for i, (freq, color) in enumerate(zip(quantized_freqs[:4], colors[:4])):  # First 4 notes\n",
    "    note_audio = synthesizer.generate_tone(freq, 1.0, method='additive')\n",
    "    note_name = mapper.frequency_to_note_name(freq)\n",
    "    \n",
    "    print(f\"Note {i+1}: {freq:.1f}Hz ({note_name}) - Color: RGB{tuple(color)}\")\n",
    "    \n",
    "    # Display audio player (if in Jupyter)\n",
    "    if HAS_WIDGETS:\n",
    "        display(HTML(f\"<h4>üéµ {note_name} ({freq:.1f}Hz)</h4>\"))\n",
    "        display(Audio(note_audio, rate=synthesizer.sample_rate))\n",
    "\n",
    "# Create a chord progression\n",
    "print(\"\\\\nüéπ Creating chord progression...\")\n",
    "chord_freqs_list = []\n",
    "\n",
    "# Use the chord progression we generated earlier\n",
    "for chord_indices in chord_progression:\n",
    "    chord_freqs = [quantized_freqs[idx] for idx in chord_indices if idx < len(quantized_freqs)]\n",
    "    chord_freqs_list.append(chord_freqs)\n",
    "\n",
    "# Generate chord audio\n",
    "full_progression = []\n",
    "for i, chord_freqs in enumerate(chord_freqs_list):\n",
    "    if chord_freqs:  # Only create chord if we have frequencies\n",
    "        chord_audio = synthesizer.create_chord(chord_freqs, duration=2.0, method='additive')\n",
    "        chord_audio = synthesizer.add_reverb(chord_audio, delay_time=0.15, decay=0.4)\n",
    "        full_progression.append(chord_audio)\n",
    "        \n",
    "        chord_notes = [mapper.frequency_to_note_name(f) for f in chord_freqs]\n",
    "        print(f\"  Chord {i+1}: {' + '.join(chord_notes)} ({[f'{f:.1f}Hz' for f in chord_freqs]})\")\n",
    "\n",
    "if full_progression:\n",
    "    progression_audio = np.concatenate(full_progression)\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Generated chord progression: {len(progression_audio)/synthesizer.sample_rate:.1f} seconds\")\n",
    "    \n",
    "    # Visualize chord progression\n",
    "    synthesizer.visualize_waveform(progression_audio, \"Color-Derived Chord Progression\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Display audio player\n",
    "    if HAS_WIDGETS:\n",
    "        display(HTML(\"<h3>üéº Complete Chord Progression from Image Colors</h3>\"))\n",
    "        display(Audio(progression_audio, rate=synthesizer.sample_rate))\n",
    "else:\n",
    "    print(\"Could not generate chord progression with available frequencies\")\n",
    "\n",
    "print(\"\\\\nüé® ‚Üí üéµ Image-to-music conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20ff5b",
   "metadata": {},
   "source": [
    "## 7. Interactive Chromasonic Pipeline\n",
    "\n",
    "Let's put it all together in an interactive interface where you can upload images, adjust parameters, and hear the results in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveChromasonic:\n",
    "    \"\"\"Complete interactive pipeline for image-to-music conversion.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extractor = ColorExtractor()\n",
    "        self.converter = WavelengthConverter()\n",
    "        self.mapper = FrequencyMapper()\n",
    "        self.scales = MusicalScales()\n",
    "        self.synthesizer = AudioSynthesizer()\n",
    "        \n",
    "        # Current session data\n",
    "        self.current_image = None\n",
    "        self.current_colors = None\n",
    "        self.current_audio = None\n",
    "        \n",
    "    def process_image_complete(self, image_path, num_colors=8, scale='major', \n",
    "                             note_duration=0.8, synthesis_method='additive'):\n",
    "        \"\"\"Complete pipeline from image to music.\"\"\"\n",
    "        \n",
    "        print(f\"üé® Processing: {Path(image_path).name}\")\n",
    "        print(f\"Parameters: {num_colors} colors, {scale} scale, {synthesis_method} synthesis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load and extract colors\n",
    "        print(\"1Ô∏è‚É£ Loading image and extracting colors...\")\n",
    "        image = self.extractor.load_image(image_path)\n",
    "        colors, counts = self.extractor.extract_colors_kmeans(image, num_colors)\n",
    "        \n",
    "        # Step 2: Convert to wavelengths\n",
    "        print(\"2Ô∏è‚É£ Converting colors to wavelengths...\")\n",
    "        wavelengths = []\n",
    "        for color in colors:\n",
    "            wl = self.converter.rgb_to_wavelength_hue_based(color[0], color[1], color[2])\n",
    "            wavelengths.append(wl)\n",
    "        \n",
    "        # Step 3: Map to frequencies\n",
    "        print(\"3Ô∏è‚É£ Mapping wavelengths to musical frequencies...\")\n",
    "        frequencies = self.mapper.wavelengths_to_frequencies(wavelengths)\n",
    "        \n",
    "        # Step 4: Quantize to scale\n",
    "        print(\"4Ô∏è‚É£ Quantizing to musical scale...\")\n",
    "        quantized_freqs, scale_notes = self.scales.quantize_to_scale(frequencies, scale)\n",
    "        \n",
    "        # Step 5: Generate audio\n",
    "        print(\"5Ô∏è‚É£ Synthesizing audio...\")\n",
    "        melody_audio = self.synthesizer.create_melody(quantized_freqs, \n",
    "                                                     note_duration, \n",
    "                                                     synthesis_method)\n",
    "        \n",
    "        # Add reverb for polish\n",
    "        melody_audio = self.synthesizer.add_reverb(melody_audio)\n",
    "        \n",
    "        # Store results\n",
    "        self.current_image = image\n",
    "        self.current_colors = colors\n",
    "        self.current_audio = melody_audio\n",
    "        \n",
    "        # Display results\n",
    "        self._display_results(image, colors, wavelengths, frequencies, \n",
    "                            quantized_freqs, scale_notes, scale, melody_audio)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'colors': colors,\n",
    "            'wavelengths': wavelengths,\n",
    "            'frequencies': frequencies,\n",
    "            'quantized_frequencies': quantized_freqs,\n",
    "            'scale_notes': scale_notes,\n",
    "            'audio': melody_audio\n",
    "        }\n",
    "    \n",
    "    def _display_results(self, image, colors, wavelengths, frequencies, \n",
    "                        quantized_freqs, scale_notes, scale, audio):\n",
    "        \"\"\"Display comprehensive results.\"\"\"\n",
    "        \n",
    "        # Create a comprehensive visualization\n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Original image\n",
    "        ax1 = plt.subplot(3, 4, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original Image', fontweight='bold', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Color palette\n",
    "        ax2 = plt.subplot(3, 4, 2)\n",
    "        color_bar = np.array(colors)[np.newaxis, :, :] / 255.0\n",
    "        plt.imshow(color_bar)\n",
    "        plt.title('Extracted Colors', fontweight='bold', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Wavelength spectrum\n",
    "        ax3 = plt.subplot(3, 4, 3)\n",
    "        spectrum_colors = []\n",
    "        for wl in wavelengths:\n",
    "            rgb = self.converter.wavelength_to_rgb_approximation(wl)\n",
    "            spectrum_colors.append(rgb)\n",
    "        spectrum_bar = np.array(spectrum_colors)[np.newaxis, :, :] / 255.0\n",
    "        plt.imshow(spectrum_bar)\n",
    "        plt.title('Wavelength Mapping', fontweight='bold', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Frequency comparison\n",
    "        ax4 = plt.subplot(3, 4, 4)\n",
    "        x_pos = np.arange(len(frequencies))\n",
    "        plt.bar(x_pos - 0.2, frequencies, width=0.4, label='Original', alpha=0.7, color='lightblue')\n",
    "        plt.bar(x_pos + 0.2, quantized_freqs, width=0.4, label='Quantized', alpha=0.7, color='orange')\n",
    "        plt.title(f'Frequencies ({scale} scale)', fontweight='bold', fontsize=12)\n",
    "        plt.ylabel('Frequency (Hz)')\n",
    "        plt.legend()\n",
    "        plt.xticks(x_pos, [f'C{i+1}' for i in range(len(frequencies))])\n",
    "        \n",
    "        # Color-wavelength plot\n",
    "        ax5 = plt.subplot(3, 4, 5)\n",
    "        scatter = plt.scatter(wavelengths, range(len(wavelengths)), \n",
    "                             c=[c/255.0 for c in colors], s=200)\n",
    "        plt.xlabel('Wavelength (nm)')\n",
    "        plt.ylabel('Color Index')\n",
    "        plt.title('Color ‚Üí Wavelength', fontweight='bold', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Frequency-note mapping\n",
    "        ax6 = plt.subplot(3, 4, 6)\n",
    "        note_names = [self.mapper.frequency_to_note_name(f) for f in quantized_freqs]\n",
    "        bars = plt.bar(range(len(quantized_freqs)), quantized_freqs, \n",
    "                      color=[c/255.0 for c in colors])\n",
    "        plt.title('Musical Notes', fontweight='bold', fontsize=12)\n",
    "        plt.ylabel('Frequency (Hz)')\n",
    "        plt.xticks(range(len(quantized_freqs)), note_names, rotation=45)\n",
    "        \n",
    "        # Audio waveform\n",
    "        ax7 = plt.subplot(3, 4, (7, 8))\n",
    "        time = np.linspace(0, len(audio) / self.synthesizer.sample_rate, len(audio))\n",
    "        plt.plot(time, audio, linewidth=0.8, color='purple')\n",
    "        plt.title('Generated Audio Waveform', fontweight='bold', fontsize=12)\n",
    "        plt.xlabel('Time (seconds)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Color distribution pie chart\n",
    "        ax8 = plt.subplot(3, 4, 9)\n",
    "        plt.pie(counts, colors=[c/255.0 for c in colors], autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Color Distribution', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        # Musical intervals\n",
    "        ax9 = plt.subplot(3, 4, 10)\n",
    "        if len(quantized_freqs) > 1:\n",
    "            intervals = []\n",
    "            for i in range(1, len(quantized_freqs)):\n",
    "                ratio = quantized_freqs[i] / quantized_freqs[i-1]\n",
    "                semitones = 12 * np.log2(ratio)\n",
    "                intervals.append(semitones)\n",
    "            \n",
    "            plt.bar(range(len(intervals)), intervals, color='skyblue')\n",
    "            plt.title('Musical Intervals', fontweight='bold', fontsize=12)\n",
    "            plt.ylabel('Semitones')\n",
    "            plt.xlabel('Note Transition')\n",
    "        \n",
    "        # Scale visualization\n",
    "        ax10 = plt.subplot(3, 4, 11)\n",
    "        scale_intervals = self.scales.scales[scale]\n",
    "        scale_colors = plt.cm.rainbow(np.linspace(0, 1, len(scale_intervals)))\n",
    "        plt.bar(range(len(scale_intervals)), scale_intervals, color=scale_colors)\n",
    "        plt.title(f'{scale.title()} Scale', fontweight='bold', fontsize=12)\n",
    "        plt.ylabel('Semitones from Root')\n",
    "        plt.xlabel('Scale Degree')\n",
    "        \n",
    "        # Summary statistics\n",
    "        ax11 = plt.subplot(3, 4, 12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        stats_text = f\"\"\"\n",
    "üìä CHROMASONIC ANALYSIS\n",
    "        \n",
    "üé® Colors: {len(colors)}\n",
    "üåà Wavelength Range: {min(wavelengths):.0f}-{max(wavelengths):.0f}nm\n",
    "üéµ Frequency Range: {min(quantized_freqs):.0f}-{max(quantized_freqs):.0f}Hz\n",
    "üéº Scale: {scale.title()}\n",
    "üéß Duration: {len(audio)/self.synthesizer.sample_rate:.1f}s\n",
    "        \n",
    "üé∂ Generated Notes:\n",
    "{' ‚Üí '.join(note_names)}\n",
    "        \n",
    "üí´ Color Harmony:\n",
    "{self.scales.descriptions[scale]}\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.text(0.05, 0.95, stats_text, transform=ax11.transAxes, \n",
    "                fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display audio player\n",
    "        if HAS_WIDGETS:\n",
    "            display(HTML(f\"<h3>üéµ Generated Melody - {scale.title()} Scale</h3>\"))\n",
    "            display(Audio(audio, rate=self.synthesizer.sample_rate))\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\"*60)\n",
    "        print(f\"‚úÖ CHROMASONIC CONVERSION COMPLETE!\")\n",
    "        print(f\"üé® {len(colors)} colors ‚Üí üåà wavelengths ‚Üí üéµ {len(note_names)} notes\")\n",
    "        print(f\"üéº Scale: {scale.title()} | üéß Duration: {len(audio)/self.synthesizer.sample_rate:.1f}s\")\n",
    "        print(f\"üé∂ Notes: {' ‚Üí '.join(note_names)}\")\n",
    "\n",
    "# Create interactive interface\n",
    "chromasonic = InteractiveChromasonic()\n",
    "\n",
    "# Interactive widgets (if available)\n",
    "if HAS_WIDGETS:\n",
    "    print(\"üéõÔ∏è Interactive Chromasonic Interface\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create widgets\n",
    "    image_upload = widgets.FileUpload(\n",
    "        accept='.png,.jpg,.jpeg,.gif,.bmp',\n",
    "        multiple=False,\n",
    "        description='Upload Image'\n",
    "    )\n",
    "    \n",
    "    num_colors_slider = widgets.IntSlider(\n",
    "        value=8, min=3, max=15, step=1,\n",
    "        description='Colors:'\n",
    "    )\n",
    "    \n",
    "    scale_dropdown = widgets.Dropdown(\n",
    "        options=['major', 'minor', 'pentatonic', 'blues', 'chromatic', 'dorian'],\n",
    "        value='major',\n",
    "        description='Scale:'\n",
    "    )\n",
    "    \n",
    "    synthesis_dropdown = widgets.Dropdown(\n",
    "        options=['sine', 'additive', 'fm', 'sawtooth'],\n",
    "        value='additive',\n",
    "        description='Synthesis:'\n",
    "    )\n",
    "    \n",
    "    duration_slider = widgets.FloatSlider(\n",
    "        value=0.8, min=0.3, max=2.0, step=0.1,\n",
    "        description='Note Duration:'\n",
    "    )\n",
    "    \n",
    "    process_button = widgets.Button(\n",
    "        description='üé® ‚Üí üéµ Convert!',\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    # Display widgets\n",
    "    display(HTML(\"<h2>üé® Chromasonic: Interactive Image-to-Music Converter</h2>\"))\n",
    "    display(widgets.VBox([\n",
    "        image_upload,\n",
    "        widgets.HBox([num_colors_slider, scale_dropdown]),\n",
    "        widgets.HBox([synthesis_dropdown, duration_slider]),\n",
    "        process_button\n",
    "    ]))\n",
    "    \n",
    "    def on_process_click(b):\n",
    "        if image_upload.value:\n",
    "            # Save uploaded file temporarily\n",
    "            filename = list(image_upload.value.keys())[0]\n",
    "            content = image_upload.value[filename]['content']\n",
    "            \n",
    "            temp_path = f\"/tmp/{filename}\"\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            # Process the image\n",
    "            try:\n",
    "                result = chromasonic.process_image_complete(\n",
    "                    temp_path,\n",
    "                    num_colors=num_colors_slider.value,\n",
    "                    scale=scale_dropdown.value,\n",
    "                    note_duration=duration_slider.value,\n",
    "                    synthesis_method=synthesis_dropdown.value\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing image: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Please upload an image first!\")\n",
    "    \n",
    "    process_button.on_click(on_process_click)\n",
    "\n",
    "else:\n",
    "    # Fallback demo with sample image\n",
    "    print(\"üé® Demo Mode: Processing sample image...\")\n",
    "    print(\"(Install ipywidgets for interactive interface)\")\n",
    "    \n",
    "    # Use the sample image we created earlier\n",
    "    sample_path = \"/tmp/chromasonic_sample.png\"\n",
    "    Image.fromarray(sample_image).save(sample_path)\n",
    "    \n",
    "    # Process with different scales\n",
    "    scales_to_test = ['major', 'minor', 'pentatonic', 'blues']\n",
    "    \n",
    "    for scale in scales_to_test:\n",
    "        print(f\"\\\\n{'='*20} {scale.upper()} SCALE {'='*20}\")\n",
    "        result = chromasonic.process_image_complete(\n",
    "            sample_path, \n",
    "            num_colors=6, \n",
    "            scale=scale,\n",
    "            note_duration=0.6,\n",
    "            synthesis_method='additive'\n",
    "        )\n",
    "\n",
    "print(\"\\\\nüéâ Chromasonic notebook complete!\")\n",
    "print(\"üî¨ You've seen the complete pipeline from image pixels to musical notes!\")\n",
    "print(\"üé® Try uploading your own images to create unique musical compositions!\")\n",
    "print(\"\\\\nüí° Key Concepts Demonstrated:\")\n",
    "print(\"   ‚Ä¢ K-means clustering for color extraction\")\n",
    "print(\"   ‚Ä¢ HSV color space and wavelength mapping\")  \n",
    "print(\"   ‚Ä¢ Musical scale theory and note quantization\")\n",
    "print(\"   ‚Ä¢ Audio synthesis techniques (sine, FM, additive)\")\n",
    "print(\"   ‚Ä¢ ADSR envelopes and audio effects\")\n",
    "print(\"   ‚Ä¢ Interactive data visualization\")\n",
    "print(\"\\\\nüöÄ Next steps: Train ML models on musical datasets for even better melodies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae695f4",
   "metadata": {},
   "source": [
    "## üîÄ Advanced Fusion Strategies\n",
    "\n",
    "Now let's explore the sophisticated fusion methods that blend color-derived notes with AI-generated melodies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42997ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÄ Fusion Strategy Demonstration\n",
    "\n",
    "def demonstrate_fusion_strategies():\n",
    "    \"\"\"Demonstrate different fusion modes and their effects on musical output.\"\"\"\n",
    "    \n",
    "    print(\"üéØ Demonstrating Advanced Fusion Strategies...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use our previously extracted colors and create sample melodies\n",
    "    wavelength_converter = WavelengthConverter()\n",
    "    melody_generator = MelodyGenerator(model_type=\"markov\")\n",
    "    \n",
    "    # Convert colors to musical notes\n",
    "    wavelengths = wavelength_converter.rgb_to_wavelengths(extracted_colors)\n",
    "    frequencies = wavelength_converter.wavelengths_to_frequencies(wavelengths)\n",
    "    color_notes = pipeline.melody_generator._frequencies_to_scale_notes(frequencies, \"major\")\n",
    "    \n",
    "    # Generate a baseline melody from AI model\n",
    "    model_melody = melody_generator.generate_melody(\n",
    "        frequencies, duration=16.0, scale=\"major\"\n",
    "    )\n",
    "    model_notes = model_melody['notes'][:16]  # Take first 16 notes\n",
    "    \n",
    "    print(f\"üé® Color-derived notes: {color_notes}\")\n",
    "    print(f\"ü§ñ AI model notes: {model_notes[:8]}...\")  # Show first 8\n",
    "    \n",
    "    # Test all fusion modes\n",
    "    fusion_modes = [FusionMode.HARD, FusionMode.SOFT, FusionMode.WEIGHTED, \n",
    "                   FusionMode.ALTERNATING, FusionMode.HARMONIC]\n",
    "    \n",
    "    scale_intervals = [0, 2, 4, 5, 7, 9, 11]  # Major scale\n",
    "    \n",
    "    fusion_results = {}\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, mode in enumerate(fusion_modes):\n",
    "        print(f\"\\nüîÄ Testing {mode.value.upper()} fusion...\")\n",
    "        \n",
    "        # Initialize fusion layer\n",
    "        fusion_layer = FusionLayer(mode)\n",
    "        \n",
    "        # Apply fusion\n",
    "        fused_notes = fusion_layer.fuse(\n",
    "            color_notes=color_notes,\n",
    "            model_notes=model_notes,\n",
    "            scale_intervals=scale_intervals,\n",
    "            weights=color_weights\n",
    "        )\n",
    "        \n",
    "        fusion_results[mode.value] = fused_notes\n",
    "        \n",
    "        # Visualize the fusion result\n",
    "        x = range(len(fused_notes))\n",
    "        axes[i].plot(x, model_notes[:len(fused_notes)], 'b--', label='Original AI', linewidth=2, alpha=0.7)\n",
    "        axes[i].scatter(range(len(color_notes)), color_notes, c='red', s=100, label='Color Notes', zorder=5)\n",
    "        axes[i].plot(x, fused_notes, 'g-', label='Fused Result', linewidth=3)\n",
    "        \n",
    "        axes[i].set_title(f'üîÄ {mode.value.capitalize()} Fusion', fontweight='bold', fontsize=12)\n",
    "        axes[i].set_xlabel('Note Position')\n",
    "        axes[i].set_ylabel('Scale Note Index')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_ylim(-1, max(max(model_notes), max(color_notes)) + 1)\n",
    "    \n",
    "    # Comparison metrics in the last subplot\n",
    "    axes[5].axis('off')\n",
    "    \n",
    "    # Calculate fusion quality metrics  \n",
    "    from chromasonic.fusion import FusionAnalyzer\n",
    "    analyzer = FusionAnalyzer()\n",
    "    \n",
    "    quality_data = []\n",
    "    for mode_name, fused_notes in fusion_results.items():\n",
    "        quality = analyzer.analyze_fusion_quality(\n",
    "            original_notes=model_notes[:len(fused_notes)],\n",
    "            fused_notes=fused_notes,\n",
    "            color_notes=color_notes\n",
    "        )\n",
    "        quality_data.append({\n",
    "            'Fusion Mode': mode_name.capitalize(),\n",
    "            'Color Preservation': f\"{quality['color_preservation']:.2f}\",\n",
    "            'Musical Coherence': f\"{quality['melodic_coherence']:.2f}\",\n",
    "            'Overall Quality': f\"{quality['overall_quality']:.2f}\"\n",
    "        })\n",
    "    \n",
    "    quality_df = pd.DataFrame(quality_data)\n",
    "    \n",
    "    # Create quality comparison table\n",
    "    table = axes[5].table(\n",
    "        cellText=quality_df.values,\n",
    "        colLabels=quality_df.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    axes[5].set_title('üìä Fusion Quality Metrics', fontweight='bold', fontsize=12, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Demonstrate adaptive fusion\n",
    "    print(f\"\\nüß† Demonstrating Adaptive Fusion...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Extract image features for adaptive decision\n",
    "    feature_extractor = ImageFeatureExtractor()\n",
    "    image_features = feature_extractor.extract_all_features(sample_image)\n",
    "    \n",
    "    # Use adaptive fusion\n",
    "    adaptive_fusion = AdaptiveFusion()\n",
    "    adaptive_notes, selected_mode = adaptive_fusion.fuse(\n",
    "        color_notes=color_notes,\n",
    "        model_notes=model_notes,\n",
    "        scale_intervals=scale_intervals,\n",
    "        image_features=image_features,\n",
    "        weights=color_weights\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Adaptive fusion selected: {selected_mode.value.upper()}\")\n",
    "    print(f\"üìà Image characteristics that influenced selection:\")\n",
    "    print(f\"   - Color harmony: {image_features.get('color_harmony_score', 0):.2f}\")\n",
    "    print(f\"   - Complexity: {image_features.get('complexity_score', 0):.2f}\")  \n",
    "    print(f\"   - Brightness: {image_features.get('brightness', 0):.2f}\")\n",
    "    print(f\"   - Saturation: {image_features.get('mean_saturation', 0):.2f}\")\n",
    "    \n",
    "    return fusion_results, adaptive_notes, selected_mode\n",
    "\n",
    "# Run fusion demonstration\n",
    "fusion_results, adaptive_melody, adaptive_mode = demonstrate_fusion_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965f0ff",
   "metadata": {},
   "source": [
    "## üìä Comprehensive Pipeline Evaluation\n",
    "\n",
    "Let's evaluate the quality of our image-to-music conversion using advanced metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Complete Pipeline Evaluation and Benchmarking\n",
    "\n",
    "def comprehensive_evaluation_demo():\n",
    "    \"\"\"Demonstrate comprehensive evaluation of the entire pipeline.\"\"\"\n",
    "    \n",
    "    print(\"üìä Comprehensive Pipeline Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Run complete pipeline with timing\n",
    "    import time\n",
    "    \n",
    "    processing_times = {}\n",
    "    \n",
    "    print(\"‚è±Ô∏è  Running complete pipeline with timing...\")\n",
    "    \n",
    "    # Time each component\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Image processing\n",
    "    comp_start = time.time()\n",
    "    # (Already done - sample_image, extracted_colors)\n",
    "    processing_times['image_processing'] = time.time() - comp_start\n",
    "    \n",
    "    # Color analysis  \n",
    "    comp_start = time.time()\n",
    "    wavelength_converter = WavelengthConverter()\n",
    "    wavelengths = wavelength_converter.rgb_to_wavelengths(extracted_colors)\n",
    "    frequencies = wavelength_converter.wavelengths_to_frequencies(wavelengths)\n",
    "    processing_times['wavelength_mapping'] = time.time() - comp_start\n",
    "    \n",
    "    # Melody generation\n",
    "    comp_start = time.time()\n",
    "    melody_result = pipeline.melody_generator.generate_melody(\n",
    "        frequencies, duration=20.0, scale=\"major\"\n",
    "    )\n",
    "    generated_melody = melody_result['notes']\n",
    "    processing_times['melody_generation'] = time.time() - comp_start\n",
    "    \n",
    "    # Audio synthesis\n",
    "    comp_start = time.time()\n",
    "    audio_data = pipeline.audio_synthesizer.synthesize(melody_result)\n",
    "    processing_times['audio_synthesis'] = time.time() - comp_start\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    processing_times['total_pipeline'] = total_time\n",
    "    \n",
    "    print(f\"‚úÖ Pipeline completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Initialize comprehensive evaluator\n",
    "    evaluator = ComprehensiveEvaluator()\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    scale_intervals = [0, 2, 4, 5, 7, 9, 11]  # Major scale\n",
    "    \n",
    "    evaluation_report = evaluator.evaluate_complete_pipeline(\n",
    "        colors=extracted_colors,\n",
    "        wavelengths=wavelengths,\n",
    "        frequencies=frequencies,\n",
    "        melody=generated_melody,\n",
    "        scale=scale_intervals,\n",
    "        processing_times=processing_times,\n",
    "        user_feedback={'satisfaction': 0.8, 'creativity': 0.75, 'musicality': 0.7}  # Simulated\n",
    "    )\n",
    "    \n",
    "    # Visualize evaluation results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Musical Quality Metrics\n",
    "    musical_metrics = evaluation_report['musical_quality']\n",
    "    metrics_names = list(musical_metrics.keys())\n",
    "    metrics_values = list(musical_metrics.values())\n",
    "    \n",
    "    axes[0, 0].barh(metrics_names, metrics_values, color='skyblue')\n",
    "    axes[0, 0].set_xlabel('Score (0-1)')\n",
    "    axes[0, 0].set_title('üéº Musical Quality Metrics', fontweight='bold')\n",
    "    axes[0, 0].set_xlim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(metrics_values):\n",
    "        axes[0, 0].text(v + 0.01, i, f'{v:.2f}', va='center')\n",
    "    \n",
    "    # 2. Color-Music Alignment\n",
    "    alignment_metrics = evaluation_report['alignment_quality']\n",
    "    alignment_names = list(alignment_metrics.keys())\n",
    "    alignment_values = list(alignment_metrics.values())\n",
    "    \n",
    "    axes[0, 1].bar(alignment_names, alignment_values, color='lightcoral')\n",
    "    axes[0, 1].set_ylabel('Score (0-1)')\n",
    "    axes[0, 1].set_title('üé® Color-Music Alignment', fontweight='bold')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(alignment_values):\n",
    "        axes[0, 1].text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    # 3. Processing Time Breakdown\n",
    "    time_names = list(processing_times.keys())\n",
    "    time_values = list(processing_times.values())\n",
    "    \n",
    "    # Create pie chart for time distribution\n",
    "    axes[1, 0].pie(time_values[:-1], labels=time_names[:-1], autopct='%1.1f%%', startangle=90)\n",
    "    axes[1, 0].set_title('‚è±Ô∏è Processing Time Breakdown', fontweight='bold')\n",
    "    \n",
    "    # 4. Overall Scores Radar Chart\n",
    "    overall_scores = evaluation_report['overall']\n",
    "    categories = ['Musical\\\\nQuality', 'Color-Music\\\\nAlignment', 'System\\\\nPerformance', 'Final\\\\nScore']\n",
    "    values = [\n",
    "        overall_scores['musical_score'],\n",
    "        overall_scores['alignment_score'], \n",
    "        overall_scores['system_score'],\n",
    "        overall_scores['final_score']\n",
    "    ]\n",
    "    \n",
    "    # Simple bar chart instead of radar for compatibility\n",
    "    axes[1, 1].bar(categories, values, color=['gold', 'lightgreen', 'lightblue', 'orange'])\n",
    "    axes[1, 1].set_ylabel('Score (0-1)')\n",
    "    axes[1, 1].set_title('üèÜ Overall Performance', fontweight='bold')\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(values):\n",
    "        axes[1, 1].text(i, v + 0.02, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed evaluation report\n",
    "    print(f\"\\nüìà EVALUATION REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üéº Musical Quality Score: {evaluation_report['overall']['musical_score']:.2f}\")\n",
    "    print(f\"üé® Color-Music Alignment: {evaluation_report['overall']['alignment_score']:.2f}\")\n",
    "    print(f\"‚ö° System Performance: {evaluation_report['overall']['system_score']:.2f}\")\n",
    "    print(f\"üèÜ FINAL SCORE: {evaluation_report['overall']['final_score']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(evaluation_report['recommendations'], 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Performance Breakdown:\")\n",
    "    for component, timing in processing_times.items():\n",
    "        print(f\"   - {component.replace('_', ' ').title()}: {timing:.3f}s\")\n",
    "    \n",
    "    # Quality comparison across different scales\n",
    "    print(f\"\\nüéµ Scale Comparison Analysis...\")\n",
    "    scale_comparison = {}\n",
    "    scales_to_test = ['major', 'minor', 'pentatonic', 'blues']\n",
    "    \n",
    "    for scale_name in scales_to_test:\n",
    "        pipeline.update_scale(scale_name)\n",
    "        scale_melody = pipeline.melody_generator.generate_melody(\n",
    "            frequencies, duration=10.0, scale=scale_name\n",
    "        )\n",
    "        \n",
    "        # Quick musical quality evaluation\n",
    "        scale_intervals_map = {\n",
    "            'major': [0, 2, 4, 5, 7, 9, 11],\n",
    "            'minor': [0, 2, 3, 5, 7, 8, 10], \n",
    "            'pentatonic': [0, 2, 4, 7, 9],\n",
    "            'blues': [0, 3, 5, 6, 7, 10]\n",
    "        }\n",
    "        \n",
    "        musical_quality = evaluator.musical_metrics.evaluate_melody(\n",
    "            scale_melody['notes'], scale_intervals_map[scale_name]\n",
    "        )\n",
    "        scale_comparison[scale_name] = musical_quality['overall_quality']\n",
    "    \n",
    "    # Display scale comparison\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    scales = list(scale_comparison.keys())\n",
    "    scores = list(scale_comparison.values())\n",
    "    \n",
    "    bars = ax.bar(scales, scores, color=['gold', 'silver', 'bronze', 'lightblue'])\n",
    "    ax.set_ylabel('Musical Quality Score')\n",
    "    ax.set_title('üéµ Musical Scale Quality Comparison', fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "               f'{score:.2f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Highlight best scale\n",
    "    best_scale = max(scale_comparison.keys(), key=scale_comparison.get)\n",
    "    best_idx = scales.index(best_scale)\n",
    "    bars[best_idx].set_color('gold')\n",
    "    bars[best_idx].set_edgecolor('orange')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üèÜ Best performing scale: {best_scale.upper()} (Score: {scale_comparison[best_scale]:.2f})\")\n",
    "    \n",
    "    return evaluation_report, scale_comparison\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "eval_report, scale_comparison = comprehensive_evaluation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b6d1d",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Image Input Widget\n",
    "\n",
    "Use this interactive widget to upload images directly in the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f437df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéÆ Interactive Image Input & Music Generation Widget\n",
    "\n",
    "def create_interactive_chromasonic():\n",
    "    \"\"\"Create an interactive widget for easy image input and music generation.\"\"\"\n",
    "    \n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, Audio, Image as IPImage\n",
    "    import io\n",
    "    import base64\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"üéÆ Chromasonic Interactive Widget\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # File upload widget\n",
    "    uploader = widgets.FileUpload(\n",
    "        accept='image/*',\n",
    "        multiple=False,\n",
    "        description='üìÅ Choose Image',\n",
    "        style={'button_color': 'lightblue'},\n",
    "        layout=widgets.Layout(width='300px')\n",
    "    )\n",
    "    \n",
    "    # Parameter controls\n",
    "    scale_selector = widgets.Dropdown(\n",
    "        options=['major', 'minor', 'pentatonic', 'blues', 'chromatic', 'dorian'],\n",
    "        value='major',\n",
    "        description='üéµ Scale:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    tempo_slider = widgets.IntSlider(\n",
    "        value=120,\n",
    "        min=60,\n",
    "        max=180,\n",
    "        step=10,\n",
    "        description='üéº Tempo:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    colors_slider = widgets.IntSlider(\n",
    "        value=8,\n",
    "        min=3,\n",
    "        max=12,\n",
    "        step=1,\n",
    "        description='üé® Colors:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    duration_slider = widgets.FloatSlider(\n",
    "        value=15.0,\n",
    "        min=5.0,\n",
    "        max=60.0,\n",
    "        step=5.0,\n",
    "        description='‚è±Ô∏è Duration:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    model_selector = widgets.Dropdown(\n",
    "        options=['markov', 'lstm', 'transformer'],\n",
    "        value='markov',\n",
    "        description='üß† AI Model:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Generate button\n",
    "    generate_btn = widgets.Button(\n",
    "        description='üéµ Generate Music!',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='200px', height='50px')\n",
    "    )\n",
    "    \n",
    "    # Output areas\n",
    "    image_output = widgets.Output()\n",
    "    color_output = widgets.Output()\n",
    "    audio_output = widgets.Output()\n",
    "    analysis_output = widgets.Output()\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üìÇ Image Input</h3>\"),\n",
    "        uploader,\n",
    "        widgets.HTML(\"<h3>üéõÔ∏è Music Parameters</h3>\"),\n",
    "        widgets.HBox([scale_selector, model_selector]),\n",
    "        widgets.HBox([tempo_slider, colors_slider]),\n",
    "        duration_slider,\n",
    "        widgets.HTML(\"<br>\"),\n",
    "        generate_btn\n",
    "    ])\n",
    "    \n",
    "    outputs = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üñºÔ∏è Uploaded Image</h3>\"),\n",
    "        image_output,\n",
    "        widgets.HTML(\"<h3>üé® Extracted Colors</h3>\"),\n",
    "        color_output,\n",
    "        widgets.HTML(\"<h3>üéµ Generated Music</h3>\"),\n",
    "        audio_output,\n",
    "        widgets.HTML(\"<h3>üìä Analysis</h3>\"),\n",
    "        analysis_output\n",
    "    ])\n",
    "    \n",
    "    main_layout = widgets.HBox([controls, outputs])\n",
    "    \n",
    "    def on_generate_click(b):\n",
    "        \"\"\"Handle music generation when button is clicked.\"\"\"\n",
    "        \n",
    "        if not uploader.value:\n",
    "            with audio_output:\n",
    "                audio_output.clear_output()\n",
    "                print(\"‚ö†Ô∏è Please upload an image first!\")\n",
    "            return\n",
    "        \n",
    "        with audio_output:\n",
    "            audio_output.clear_output()\n",
    "            print(\"üéµ Generating music... Please wait!\")\n",
    "        \n",
    "        try:\n",
    "            # Get uploaded image\n",
    "            uploaded_file = list(uploader.value.values())[0]\n",
    "            image_data = uploaded_file['content']\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            image_array = np.array(image.convert('RGB'))\n",
    "            \n",
    "            # Display image\n",
    "            with image_output:\n",
    "                image_output.clear_output()\n",
    "                display(IPImage(data=image_data, width=300))\n",
    "            \n",
    "            # Save temporary image\n",
    "            temp_path = '/tmp/uploaded_image.png'\n",
    "            image.save(temp_path)\n",
    "            \n",
    "            # Update pipeline parameters\n",
    "            pipeline.update_scale(scale_selector.value)\n",
    "            pipeline.update_tempo(tempo_slider.value)\n",
    "            pipeline.duration = duration_slider.value\n",
    "            \n",
    "            # Process image\n",
    "            result = pipeline.process_image(\n",
    "                temp_path,\n",
    "                num_colors=colors_slider.value\n",
    "            )\n",
    "            \n",
    "            # Display color palette\n",
    "            with color_output:\n",
    "                color_output.clear_output()\n",
    "                colors = result['colors']\n",
    "                \n",
    "                # Create color swatches\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "                \n",
    "                for i, (r, g, b) in enumerate(colors):\n",
    "                    rect = plt.Rectangle((i, 0), 1, 1, color=(r/255, g/255, b/255))\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(i+0.5, 0.5, f'RGB({r},{g},{b})', \n",
    "                           ha='center', va='center', fontsize=8, \n",
    "                           color='white' if (r+g+b) < 384 else 'black')\n",
    "                \n",
    "                ax.set_xlim(0, len(colors))\n",
    "                ax.set_ylim(0, 1)\n",
    "                ax.set_title(f'üé® {len(colors)} Extracted Colors')\n",
    "                ax.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Display audio and analysis\n",
    "            with audio_output:\n",
    "                audio_output.clear_output()\n",
    "                \n",
    "                # Save audio to temporary file\n",
    "                import tempfile\n",
    "                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_audio:\n",
    "                    pipeline.save_audio(result['audio'], tmp_audio.name)\n",
    "                    \n",
    "                    # Display audio player\n",
    "                    display(HTML(f\"<h4>üéµ Generated Music ({duration_slider.value}s)</h4>\"))\n",
    "                    display(Audio(tmp_audio.name))\n",
    "                    \n",
    "                    print(f\"‚úÖ Music generated successfully!\")\n",
    "                    print(f\"   Scale: {scale_selector.value}\")\n",
    "                    print(f\"   Tempo: {tempo_slider.value} BPM\") \n",
    "                    print(f\"   Model: {model_selector.value}\")\n",
    "            \n",
    "            # Display analysis\n",
    "            with analysis_output:\n",
    "                analysis_output.clear_output()\n",
    "                \n",
    "                # Quick analysis\n",
    "                wavelengths = result['wavelengths']\n",
    "                frequencies = result['frequencies']\n",
    "                \n",
    "                print(\"üìä Technical Analysis:\")\n",
    "                print(f\"   üåà Wavelength range: {min(wavelengths):.0f}-{max(wavelengths):.0f} nm\")\n",
    "                print(f\"   üéµ Frequency range: {min(frequencies):.0f}-{max(frequencies):.0f} Hz\")\n",
    "                print(f\"   üéº Melody length: {len(result['melody']['notes'])} notes\")\n",
    "                print(f\"   ‚è±Ô∏è Audio duration: {len(result['audio'])/44100:.1f} seconds\")\n",
    "                \n",
    "                # Show wavelength to color mapping\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                \n",
    "                # Wavelength visualization\n",
    "                ax1.bar(range(len(wavelengths)), wavelengths, \n",
    "                       color=[f'#{r:02x}{g:02x}{b:02x}' for r, g, b in colors])\n",
    "                ax1.set_title('üåà Color Wavelengths (nm)')\n",
    "                ax1.set_ylabel('Wavelength (nm)')\n",
    "                ax1.set_xlabel('Color Index')\n",
    "                \n",
    "                # Frequency visualization  \n",
    "                ax2.bar(range(len(frequencies)), frequencies,\n",
    "                       color=[f'#{r:02x}{g:02x}{b:02x}' for r, g, b in colors])\n",
    "                ax2.set_title('üéµ Musical Frequencies (Hz)')\n",
    "                ax2.set_ylabel('Frequency (Hz)')\n",
    "                ax2.set_xlabel('Color Index')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            with audio_output:\n",
    "                audio_output.clear_output()\n",
    "                print(f\"‚ùå Error generating music: {e}\")\n",
    "    \n",
    "    # Connect button to function\n",
    "    generate_btn.on_click(on_generate_click)\n",
    "    \n",
    "    # Display widget\n",
    "    display(main_layout)\n",
    "    \n",
    "    # Instructions\n",
    "    display(HTML(\"\"\"\n",
    "    <div style=\"background: #e8f4fd; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
    "        <h4>üéØ How to Use:</h4>\n",
    "        <ol>\n",
    "            <li><strong>üìÅ Upload Image:</strong> Click \"Choose Image\" and select any image file</li>\n",
    "            <li><strong>üéõÔ∏è Adjust Parameters:</strong> Choose your preferred scale, tempo, and other settings</li>  \n",
    "            <li><strong>üéµ Generate:</strong> Click \"Generate Music!\" to create your melody</li>\n",
    "            <li><strong>üéß Listen:</strong> Play the generated audio directly in the notebook</li>\n",
    "        </ol>\n",
    "        <p><em>üí° Tip: Try different scales with the same image to hear how they change the mood!</em></p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "# Try to create the widget (requires ipywidgets)\n",
    "try:\n",
    "    create_interactive_chromasonic()\n",
    "except ImportError:\n",
    "    print(\"üìù Note: Install ipywidgets for the interactive widget:\")\n",
    "    print(\"   pip install ipywidgets\")\n",
    "    print(\"   jupyter nbextension enable --py widgetsnbextension\")\n",
    "    print(\"\")\n",
    "    print(\"üîÑ Alternative: Use the simple file-based approach below:\")\n",
    "    \n",
    "    # Simple file upload demonstration\n",
    "    def simple_image_demo():\n",
    "        \"\"\"Simple demo without widgets - just drag files to the notebook folder.\"\"\"\n",
    "        \n",
    "        print(\"üìÇ Simple Image Input Method:\")\n",
    "        print(\"=\" * 35)\n",
    "        print(\"1. üìÅ Place your image in: chromasonic/data/images/\")\n",
    "        print(\"2. üñºÔ∏è Or use the sample image we created:\")\n",
    "        \n",
    "        # Process the sample image we created earlier\n",
    "        if Path('../data/images/test_sunset.png').exists():\n",
    "            print(\"   ‚úÖ Found sample image: test_sunset.png\")\n",
    "            \n",
    "            # Quick demo\n",
    "            result = pipeline.process_image('../data/images/test_sunset.png')\n",
    "            \n",
    "            print(f\"   üé® Extracted {len(result['colors'])} colors\")\n",
    "            print(f\"   üéµ Generated {len(result['melody']['notes'])} note melody\")\n",
    "            print(f\"   ‚è±Ô∏è Audio length: {len(result['audio'])/44100:.1f}s\")\n",
    "            \n",
    "            # Display colors as text\n",
    "            print(f\"   üåà Colors: {[f'RGB{color}' for color in result['colors'][:3]]}...\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No sample image found - create one first!\")\n",
    "            return None\n",
    "    \n",
    "    simple_image_demo()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
